# Mapping Fusion using Mixture-of-Experts (MoE)

This directory contains the data, implementation, and output results for **Stage 3** of our framework, which focuses on fusing code-to-architecture mappings using a **Mixture-of-Experts (MoE)** model. This stage aims to integrate mapping results produced by different methods and enhance overall prediction accuracy through dynamic expert weighting.

## Directory Contents

| Filename                                                     | Type   | Description                                                  |
| ------------------------------------------------------------ | ------ | ------------------------------------------------------------ |
| `Diverse_mapping_results_for_MoE.csv`                        | Data   | Raw mapping results generated by three individual methods (BM25, LLM-S, IP-RAG-A) for each file, serving as input sources for fusion. |
| `MoE_input.csv`                                              | Data   | A processed binary classification dataset derived from the above file, indicating whether a given file-to-module mapping belongs to a Documented Module (DM) or an Undocumented Module (UM). |
| `predict.py`                                                 | Script | The main Python script for training the MoE model and producing predictions on the test set. |
| `predictions.csv`                                            | Output | The prediction results generated by the MoE model, specifying the classification (DM/UM) of each mapping instance. |
| `hadoop_best.pth` `jabref_best.pth` `mediastore_best.pth` `teammates_best.pth` `teastore_best.pth` | Model  | Pretrained MoE model weights (in PyTorch format) for each corresponding software system. |



## Purpose

This stage addresses the inconsistencies and limitations of individual mapping methods by leveraging a Mixture-of-Experts architecture. It performs the following key functions:

- Aggregates diverse mapping outputs;
- Trains a binary classifier to distinguish between DM and UM mappings;
- Produces fused mappings for use in the final documentation generation phase.

## Usage Instructions

1. **Data Preparation**
    Ensure that the file `Diverse_mapping_results_for_MoE.csv` is available and contains raw mapping results from the three base methods.

2. **Model Training and Prediction**
    Execute the training and inference process using the provided script:

   ```bash
   bash
   
   python predict.py
   ```

   This will train the MoE model using `MoE_input.csv` and generate predictions.

3. **Output**
    The final classification results will be saved in `predictions.csv`.